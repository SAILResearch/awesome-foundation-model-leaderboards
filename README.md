<div align="center">
    <h1>Awesome Foundation Model Leaderboard</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

**Awesome Foundation Model Leaderboard** is a curated list of awesome leaderboards for foundation models organized according to [our survey](TDOO):

<p align="center"><strong>An Exploratory Study on the Architecture, Operations, and Smells of Leaderboard System for Foundation Models</strong></p>

<p align="center"><a href="https://github.com/zhimin-z">Zhimin Zhao</a>†, <a href="https://abdulali.github.io">Abdul Ali Bangash</a>, <a href="https://www.filipecogo.pro">Filipe Roseiro Côgo</a>, <a href="https://mcis.cs.queensu.ca/bram.html">Bram Adams</a>, <a href="https://research.cs.queensu.ca/home/ahmed">Ahmed E. Hassan</a></p>

<p align="center">Queen's University</p>

<p align="center">(†: Corresponding author)</p>

If you find our survey useful, please kindly cite our paper:

```bibtex
TODO
```

_If you want to contribute to this list (please do), welcome to propose a pull request._

_If you have any suggestions, critiques, or questions regarding this list, welcome to raise an issue._

Also, a listed leaderboard should be included if only:

* It is popular enough.
* It is actively maintained.
* It is related to machine learning.

# Tool

| Name | Description |
| ---- | ----------- |
| [Leaderboard Explorer](https://huggingface.co/spaces/leaderboards/LeaderboardFinder) | Leaderboard Explorer is a tool to assist users in navigating through the diverse range of leaderboards available on Hugging Face Spaces. |
| [Open Leaderboards Leaderboard](https://huggingface.co/spaces/leaderboards/LeaderboardFinder) | Open Leaderboards Leaderboard is a meta-leaderboard that leverages human preferences to compare machine learning leaderboards. |

# General

| Name | Description |
| ---- | ----------- |
| [Allen Institute for AI](https://leaderboard.allenai.org) | Allen Institute for AI is a non-profit research institute with the mission of conducting high-impact AI research and engineering in service of the common good. |
| [Robust Reading Competition](https://rrc.cvc.uab.es) | Robust Reading refers to the research area dealing with the interpretation of written communication in unconstrained settings. |

# Text

| Name | Description |
| ---- | ----------- |
| [ACLUE](https://github.com/isen-zhang/ACLUE) | ACLUE is an evaluation benchmark for ancient Chinese language comprehension. |
| [AgentBench](https://llmbench.ai/agent/data) | AgentBench is the first benchmark to evaluate language model-as-Agent across a diverse spectrum of different environments. |
| [AlignBench](https://llmbench.ai/align/data) | AlignBench is a multi-dimensional benchmark for evaluating language models' alignment in Chinese. |
| [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval) | AlpacaEval is an automatic evaluator designed for instruction-following language models. |
| [ANGO](https://huggingface.co/spaces/AngoHF/ANGO-Leaderboard) | ANGO is a generation-oriented Chinese language model evaluation benchmark. |
| [Biomedical Knowledge Probing Leaderboard](https://huggingface.co/spaces/CDT-BMAI-GP/biomed_probing_leaderboard) | Biomedical Knowledge Probing Leaderboard aims to track, rank, and evaluate biomedical factual knowledge probing results in language models. |
| [BotChat](https://botchat.opencompass.org.cn) | BotChat assesses the multi-round chatting capabilities of language models through a proxy task, evaluating whether two ChatBot instances can engage in smooth and fluent conversation with each other. |
| [CFBenchmark](https://github.com/TongjiFinLab/CFBenchmark) | CFBenchmark is a benchmark to evaluate the capabilities of Chinese language model in practical financial applications. |
| [C-Eval](https://cevalbenchmark.com/static/leaderboard.html) | C-Eval is a Chinese evaluation suite for language models. |
| [ChineseFactEval](https://github.com/GAIR-NLP/factool/tree/main/datasets/chinese) | ChineseFactEval is a factuality benchmark for Chinese language models. |
| [CLEM](https://huggingface.co/spaces/colab-potsdam/clem-leaderboard) | CLEM is a framework designed for the systematic evaluation of chat-optimized language models as conversational agents. |
| [CLiB](https://github.com/jeinlee1991/chinese-llm-benchmark) | CLiB is a framework designed for the evaluation of Chinese language models. |
| [CMMLU](https://github.com/haonan-li/CMMLU) | CMMLU is a Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning capabilities of language models. |
| [CMB](https://cmedbenchmark.llmzoo.com/static/leaderboard.html) | CMB is a multi-level medical benchmark in Chinese. |
| [CMMLU](https://github.com/haonan-li/CMMLU) | CMMLU is a benchmark to evaluate the performance of language models in various subjects within the Chinese cultural context. |
| [CMMMU](https://cmmmu-benchmark.github.io/#leaderboard) | CMMMU is a benchmark to test the capabilities of multimodal models in understanding and reasoning across multiple disciplines in the Chinese context. |
| [CompMix](https://qa.mpi-inf.mpg.de/compmix) | CompMix is a benchmark for heterogeneous question answering. |
| [ConvRe](https://huggingface.co/spaces/3B-Group/ConvRe-Leaderboard) | ConvRe is a benchmark to evaluate language models' ability to comprehend converse relations. |
| [CriticBench](https://open-compass.github.io/CriticBench) | CriticBench is a benchmark to evaluate language models' ability to make critique responses. |
| [DecodingTrust](https://decodingtrust.github.io/leaderboard) | DecodingTrust is an assessment platform to evaluate the trustworthiness of language models. |
| [Enterprise Scenarios leaderboard](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard) | Enterprise Scenarios Leaderboard aims to assess the performance of language models on real-world enterprise use cases. |
| [EQ-Bench](https://eqbench.com) | EQ-Bench is a benchmark to evaluate aspects of emotional intelligence in language models. |
| [FActScore](https://github.com/shmsw25/factscore) | FActScore is an automatic evaluation metric designed to assess the factual precision in long-form text generation. |
| [Factuality Leaderboard](https://github.com/gair-nlp/factool) | Factuality Leaderboard compares the factual capabilities of LLMs. |
| [FinanceIQ](https://github.com/Duxiaoman-DI/XuanYuan/tree/main/FinanceIQ) | FinanceIQ is a Chinese evaluation dataset that assesses language models' knowledge and reasoning in financial scenarios. |
| [FuseReviews](https://huggingface.co/spaces/lovodkin93/FuseReviews-Leaderboard) | FuseReviews aims to advance grounded text generation tasks, including long-form question-answering and summarization. |
| [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) | GAIA aims to test fundamental abilities that an AI assistant should possess. |
| [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) | GAOKAO-Bench is a benchmark for evaluating language models' language understanding and logical reasoning capabilities in the context of Chinese college entrance exams. |
| [Guerra LLM AI Leaderboard](https://huggingface.co/spaces/luisrguerra/guerra-llm-ai-leaderboard) | Guerra LLM AI Leaderboard compares and ranks the performance of language models across quality, price, performance, context window, and others. |
| [Hallucinations Leaderboard](https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard) | Hallucinations Leaderboard aims to track, rank and evaluate hallucinations in language models. |
| [HalluQA](https://github.com/OpenMOSS/HalluQA) | HalluQA is a benchmark to evaluate the phenomenon of hallucinations in Chinese language models. |
| [HellaSwag](https://rowanzellers.com/hellaswag) | HellaSwag is a benchmark to evaluate common-sense reasoning in language models. |
| [HELM (Classic)](https://crfm.stanford.edu/helm/classic/latest) | HELM (Classic) is a benchmark to evaluate foundation language models. |
| [HELM (Instruct)](https://crfm.stanford.edu/helm/instruct/latest) | HELM (Instruct) is a multidimensional evaluation framework designed for assessing instruction-following language models. |
| [HELM (Lite)](https://crfm.stanford.edu/helm/lite/latest) | HELM (Lite) is a lightweight benchmark framework designed for evaluating the capabilities of language models. |
| [HHEM leaderboard](https://huggingface.co/spaces/vectara/leaderboard) | HHEM leaderboard evaluates how often an language model introduces hallucinations when summarizing a document. |
| [Indic LLM Leaderboard](https://huggingface.co/spaces/Cognitive-Lab/indic_llm_leaderboard) | Indic LLM Leaderboard is a benchmark to track progress and rank the performance of Indic language models. |
| [InstructEval](https://declare-lab.github.io/instruct-eval) | InstructEval is an evaluation suite to assess instruction selection methods in the context of language models. |
| [Japanese Chatbot Arena Leaderboard](https://huggingface.co/spaces/yutohub/japanese-chatbot-arena-leaderboard) | Japanese Chatbot Arena Leaderboard hosts the chatbot arena, where various language models compete based on their performance in Japnese. |
| [JustEval](https://allenai.github.io/re-align/just_eval.html) | JustEval is a powerful tool designed for fine-grained evaluation of language models. |
| [KAgentBench](https://github.com/kwaikeg/kwaiagents) | KAgentBench is a benchmark to test agent capabilitie, include planning, tool use, reflection, concluding, and profiling. |
| [Ko Chatbot Arena Leaderboard](https://elo.instruct.kr/leaderboard) | Ko Chatbot Arena Leaderboard hosts the chatbot arena, where various language models compete based on their performance in Korean. |
| [KoLA](http://103.238.162.37:31622/LeaderBoard) | KoLA is a benchmark to assess the world knowledge of language models. |
| [L-Eval](https://l-eval.github.io) | L-Eval is a Long Context Language Models (LCLMs) evaluation benchmark to assess the performance of handling extensive context. |
| [LAiW](https://huggingface.co/spaces/daishen/SCULAiW) | LAiW is an evaluation framework for Chinese legal language understanding and reasoning. |
| [LawBench](https://lawbench.opencompass.org.cn/leaderboard) | LawBench is a benchmark to assess the legal capabilities of language models. |
| [LLM Benchmarker Suite Leaderboard](https://llm-evals.formula-labs.com) | LLM API Hosts Leaderboard assesses language model API providers based on metrics, including price, performance, context window size, and more. |
| [LLM Safety Leaderboard](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard) | LLM Safety Leaderboard aims to provide a unified evaluation for language model safety. |
| [LLM-Leaderboard](https://github.com/LudwigStumpp/llm-leaderboard) | LLM-Leaderboard is a joint community effort to create one central leaderboard for LLMs. |
| [LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) | LLM-Perf Leaderboard aims to benchmark the performance of language models with different hardwares, backends and optimizations. |
| [LLMEval](http://llmeval.com) | LLMEval is a benchmark to assess the quality of open-domain conversations with language models. |
| [LLMHallucination Leaderboard](https://huggingface.co/spaces/ramiroluo/LLMHallucination_Leaderboard) | Hallucinations Leaderboard evaluates language models based on an array of hallucination-related benchmarks. |
| [LLMPerf](https://github.com/ray-project/llmperf-leaderboard) | LLMPerf is a tool to evaluate the performance of language models using both load and correctness tests. |
| [LMExamQA](https://lmexam.com) | LMExamQA is a benchmark to assess the performance of language models on open-ended question answering tasks. |
| [LMSYS Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) | LMSYS Chatbot Arena Leaderboard hosts the chatbot arena, where various language models compete based on their performance in English. |
| [LongBench](https://github.com/THUDM/LongBench) | LongBench is a benchmark for assessing the long context understanding capabilities of LLMs. |
| [LucyEval](http://lucyeval.besteasy.com/leaderboard.html) | LucyEval offers a thorough assessment of language models' performance in various Chinese contexts. |
| [M3KE](https://github.com/tjunlp-lab/M3KE) | M3KE is a massive multi-level multi-subject knowledge evaluation benchmark to measure the knowledge acquired by Chinese language models. |
| [MedBench](https://medbench.opencompass.org.cn/leaderboard) | MedBench is a benchmark to evaluate the mastery of knowledge and reasoning abilities in medical language models. |
| [Meta Open LLM leaderboard](https://huggingface.co/spaces/felixz/meta_open_llm_leaderboard) | The Meta Open LLM leaderboard serves as a central hub for consolidating data from various open LLM leaderboards into a single, user-friendly visualization page. |
| [MINT-Bench](https://xwang.dev/mint-bench) | MINT-Bench is a benchmark to assess the capabilities of language models in solving tasks involving multi-turn interactions. |
| [ML.ENERGY Leaderboard](https://ml.energy/leaderboard) | ML.ENERGY Leaderboard evaluates the energy consumption of language models. |
| [MMCU](https://github.com/Felixgithub2017/MMCU) | MMCU is a benchmark to evaluate the multitask accuracy of Chinese language models. |
| [MMLU](https://github.com/hendrycks/test) | MMLU is a benchmark to evaluate the performance of language models across a wide array of natural language understanding tasks. |
| [MMLU-by-task Leaderboard](https://huggingface.co/spaces/CoreyMorris/MMLU-by-task-Leaderboard) | MMLU-by-task Leaderboard provides a platform for evaluating and comparing various ML models across different language understanding tasks. |
| [Models Leaderboard](https://artificialanalysis.ai/leaderboards/models) | Models Leaderboard compares and ranks language models across key metrics, including quality, price, performance, and speed. |
| [MSTEB](https://huggingface.co/spaces/clibrain/Spanish-Embeddings-Leaderboard) | MSTEB is a benchmark for measuring the performance of text embedding models on diverse embedding tasks in Spanish. |
| [MTEB](https://huggingface.co/spaces/mteb/leaderboard) | MTEB is a massive benchmark for measuring the performance of text embedding models on diverse embedding tasks across 112 languages. |
| [MY Malay LLM Leaderboard](https://huggingface.co/spaces/mesolitica/malay-llm-leaderboard) | MY Malay LLM Leaderboard aims to track, rank, and evaluate open language models on Malay tasks. |
| [MY Malaysian Embedding Leaderboard](https://huggingface.co/spaces/mesolitica/malaysian-embedding-leaderboard) | MY Malaysian Embedding Leaderboard measures and ranks the performance of text embedding models on diverse embedding tasks in Malay. |
| [NPHardEval](https://huggingface.co/spaces/NPHardEval/NPHardEval-leaderboard) | NPHardEval is a benchmark to assess the reasoning abilities of language models through the lens of computational complexity classes. |
| [OpenCompass LLM Leaderboard](https://rank.opencompass.org.cn/leaderboard-llm-v2) | OpenCompass LLM Leaderboard is a platform for a fair, open, and reproducible large model evaluation. |
| [OpenLLM Turkish leaderboard](https://huggingface.co/spaces/malhajar/OpenLLMTurkishLeaderboard) | OpenLLM Turkish leaderboard tracks progress and ranks performance of language models in Turkish. |
| [Open Dutch LLM Evaluation Leaderboard](https://huggingface.co/spaces/BramVanroy/open_dutch_llm_leaderboard) | Open Dutch LLM Evaluation Leaderboard tracks progress and ranks performance of language models in Dutch. |
| [Open ITA LLM Leaderboard](https://huggingface.co/spaces/FinancialSupport/open_ita_llm_leaderboard) | Open ITA LLM Leaderboard tracks progress and ranks performance of language models in Italian. |
| [Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard) | Open Ko-LLM Leaderboard tracks progress and ranks performance of language models in Korean. |
| [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | Open LLM Leaderboard tracks progress and ranks performance of language models in English. |
| [Open Medical-LLM Leaderboard](https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard) | Open Medical-LLM Leaderboard aims to track, rank, and evaluate open language models in the medical domain. |
| [Open MLLM Leaderboard](https://github.com/hkust-nlp/felm) | Open MLLM Leaderboard aims to track, rank and evaluate language models and chatbots. |
| [Open MOE LLM Leaderboard](https://huggingface.co/spaces/sparse-generative-ai/open-moe-llm-leaderboard) | OPEN MOE LLM Leaderboard assesses the performance and efficiency of various Mixture of Experts (MoE) language models. |
| [Open Multilingual LLM Evaluation Leaderboard](https://huggingface.co/spaces/uonlp/open_multilingual_llm_leaderboard) | Open Multilingual LLM Evaluation Leaderboard tracks progress and ranks performance of language models in multiple languages. |
| [Open PL LLM Leaderboard](https://github.com/hkust-nlp/felm) | Open PL LLM Leaderboard is a platform for assessing the performance of various language models in Polish. |
| [Open PT LLM Leaderboard](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard) | Open PT LLM Leaderboard tracks progress and ranks the performance of language models in Portuguese. |
| [OpenEval](https://github.com/hkust-nlp/felm) | OpenEval is a multidimensional and open evaluation system to assess Chinese language models. |
| [Powered-by-Intel LLM Leaderboard](https://huggingface.co/spaces/Intel/powered_by_intel_llm_leaderboard) | Powered-by-Intel LLM Leaderboard evaluates, scores, and ranks language models that have been pre-trained or fine-tuned on Intel Hardware. |
| [PubMedQA](https://pubmedqa.github.io) | PubMedQA is a benchmark to assess biomedical research question answering. |
| [QuALITY](https://nyu-mll.github.io/quality) | QuALITY is a benchmark to assess multiple-choice question answering with long context. |
| [Red-Eval](https://github.com/declare-lab/red-instruct) | Red-Eval is a benchmark to assess the robustness of language models against harmful queries. |
| [Red Teaming Resistance Benchmark](https://huggingface.co/spaces/HaizeLabs/red-teaming-resistance-benchmark) | Red Teaming Resistance Benchmark is a benchmark to assess the robustness of language models against redteaming prompts. |
| [RoleEval](https://github.com/magnetic2014/roleeval) | RoleEval is a bilingual benchmark to assess the memorization, utilization, and reasoning capabilities of role knowledge of language models. |
| [Safety Prompts](http://coai.cs.tsinghua.edu.cn/leaderboard) | Safety Prompts is a benchmark to evaluate the safety of Chinese language models. |
| [SafetyBench](https://llmbench.ai/safety/data) | SafetyBench is a benchmark to evaluate the safety of language models. |
| [SALAD-Bench](https://huggingface.co/spaces/OpenSafetyLab/Salad-Bench-Leaderboard) | SALAD-Bench is a benchmark for evaluating safety and security of language models. |
| [ScandEval](https://scandeval.com) | ScandEval is a benchmark to evaluate language models on tasks in Scandinavian languages as well as German, Dutch, and English. |
| [SCROLLS](https://www.scrolls-benchmark.com/leaderboard) | SCROLLS is a benchmark to evaluate the reasoning capabilities of language models over long texts. |
| [SeaEval](https://huggingface.co/spaces/SeaEval/SeaEval_Leaderboard) | SeaEval is a benchmark to evaluate the performance of multilingual language models in understanding and reasoning with natural language, as well as comprehending cultural practices, nuances, and values. |
| [Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard) | Subquadratic LLM Leaderboard evaluates language models with subquadratic/attention-free architectures (i.e. RWKV & Mamba). |
| [SummEdits](https://github.com/salesforce/factualNLG) | The SummEdits is a benchmark to measure the ability of language models to reason about facts and detect inconsistencies. |
| [SuperBench](https://fm.ai.tsinghua.edu.cn/superbench) | SuperBench is a comprehensive evaluation system of tasks and dimensions to assess the overall capabilities of language models. |
| [SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE) | SuperCLUE is a benchmark to evaluate the performance of language models in Chinese. |
| [SuperCLUE-Agent](https://github.com/CLUEbenchmark/SuperCLUE-Agent) | SuperCLUE-Agent is a benchmark to evaluate the core capabilities of Chinese agents in handling native tasks. |
| [SuperCLUE-Auto](https://github.com/CLUEbenchmark/SuperCLUE-Auto) | SuperCLUE-Auto is a benchmark to evaluate Chinese language models in the automotive industry. |
| [SuperCLUE-Code3](https://github.com/CLUEbenchmark/SuperCLUE-Code3) | SuperCLUE-Code3 is a benchmark to evaluate the code generation capabilities of Chinese language models. |
| [SuperCLUE-Industry](https://github.com/CLUEbenchmark/SuperCLUE-Industry) | SuperCLUE-Industry is a benchmark specifically to evaluate Chinese language models in the context of industrial applications. |
| [SuperCLUE-Open](https://github.com/CLUEbenchmark/SuperCLUE-Open) | SuperCLUE-Open is a benchmark to evaluate Chinese language models in the context of open-domain multi-turn dialogues. |
| [SuperCLUE-Safety](https://github.com/CLUEbenchmark/SuperCLUE-Safety) | SuperCLUE-Safety is a benchmark to assess the safety and reliability of language models in Chinese. |
| [SuperCLUEgkzw](https://github.com/hkust-nlp/felm) | SuperCLUEgkzw is a benchmark for automated essay scoring in the context of Chinese college entrance exams. |
| [SuperCLUElyb](https://github.com/CLUEbenchmark/SuperCLUElyb) | SuperCLUElyb hosts the chatbot arena, where various language models compete based on their performance in Chinese. |
| [SuperGLUE](https://super.gluebenchmark.com/leaderboard) | SuperGLUE is a benchmark to evaluate the performance of language models on a set of challenging language understanding tasks. |
| [SuperLim](https://lab.kb.se/leaderboard/results) | SuperLim is a benchmark to assess the language understanding capabilities of language models in Swedish. |
| [T-Eval](https://open-compass.github.io/T-Eval/leaderboard.html) | T-Eval is a benchmark for evaluating the tool utilization capability of language models. |
| [TAT-DQA](https://nextplusplus.github.io/TAT-DQA) | TAT-DQA is a benchmark to evaluate language models on the discrete reasoning over documents that combine both structured and unstructured information. |
| [TAT-QA](https://nextplusplus.github.io/TAT-QA) | TAT-QA is a benchmark to evaluate language models on the discrete reasoning over documents that combines both tabular and textual content. |
| [The Pile](https://pile.eleuther.ai) | The Pile is a benchmark to evaluate the world knowledge and reasoning ability of language models. |
| [TheoremQA](https://github.com/wenhuchen/TheoremQA) | TheoremQA is a benchmark to evaluate language models' capabilities in applying theorems to solve challenging science problems. |
| [Toloka LLM Leaderboard](https://huggingface.co/spaces/toloka/open-llm-leaderboard) | Toloka LLM Leaderboard is a benchmark to evaluate language models based on authentic user prompts and expert human evaluation. |
| [Toolbench](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard) | ToolBench is a platform for training, serving, and evaluating language models specifically for tool learning. |
| [Toxicity Leaderboard](https://huggingface.co/spaces/Bias-Leaderboard/leaderboard) | Toxicity Leaderboard evaluates the toxicity of language models. |
| [TrustLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/leaderboard.html) | TrustLLM is a benchmark to evaluate the trustworthiness of language models. |
| [UGI Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard) | UGI Leaderboard measures and compares the uncensored and controversial information known by language models. |
| [VLLMs Leaderboard](https://huggingface.co/spaces/vlsp-2023-vllm/VLLMs-Leaderboard) | VLLMs Leaderboard aims to track, rank and evaluate open language models and chatbots. |
| [Xiezhi](https://github.com/MikeGu721/XiezhiBenchmark) | Xiezhi is a benchmark for holistic domain knowledge evaluation of language models. |
| [Yet Another LLM Leaderboard](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard) | Yet Another LLM Leaderboard is a platform for tracking, ranking, and evaluating open language models and chatbots. |

# Image

| Name | Description |
| ---- | ----------- |
| [AesBench](https://github.com/yipoh/AesBench) | AesBench is a benchmark to evaluate vision-language models on image aesthetics perception. |
| [CCBench](https://mmbench.opencompass.org.cn/leaderboard) | CCBench is a benchmark to assess the multi-modal capabilities of vision-language models specifically related to Chinese culture. |
| [ChEF](https://openlamm.github.io/Leaderboards) | ChEF is a benchmark to evaluate vision-language models across various visual reasoning tasks. |
| [ConTextual](https://huggingface.co/spaces/ucla-contextual/contextual_leaderboard) | ConTextual is a benchmark to evaluate vision-language models across context-sensitive text-rich visual reasoning tasks. |
| [CORE-MM](https://core-mm.github.io) | CORE-MM is a benchmark to evaluate the open-ended visual question answering (VQA) capabilities of vision-language models. |
| [GlitchBench](https://huggingface.co/spaces/glitchbench/Leaderboard) | GlitchBench is a benchmark to evaluate the reasoning capabilities of vision-language models in the context of detecting video game glitches. |
| [HallusionBench](https://github.com/tianyi-lab/HallusionBench) | HallusionBench is a benchmark to evaluate image-context reasoning capabilities of vision-language models. |
| [HEIM](https://crfm.stanford.edu/heim/latest) | HEIM is a benchmark to evaluate the visual and linguistic reasoning capabilities of text-to-image models. |
| [InfiMM-Eval](https://infimm.github.io/InfiMM-Eval) | InfiMM-Eval is a benchmark to evaluate the open-ended VQA capabilities of vision-language models. |
| [LVLM-eHub](http://lvlm-ehub.opengvlab.com/leaderboard.html) | LVLM-eHub is a benchmark to evaluate the visual reasoning capabilities of vision-language models. |
| [Mementos](https://mementos-bench.github.io/#leaderboard) | Mementos is a benchmark to evaluate the reasoning capabilities of vision-language models over image sequences. |
| [MLLM-Bench](https://mllm-bench.llmzoo.com/static/leaderboard.html) | MLLM-Bench is a benchmark to evaluate the visual reasoning capabitlities of MLVMs. |
| [MMBench](https://mmbench.opencompass.org.cn/leaderboard) | MMBench is a benchmark to evaluate the visual reasoning capabilities of vision-language models. |
| [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) | MME is a benchmark to evaluate the visual reasoning capabilities of vision-language models. |
| [MMMU](https://mmmu-benchmark.github.io/#leaderboard) | MMMU is a benchmark to evaluate the performance of multimodal models on tasks that demand college-level subject knowledge and expert-level reasoning across various disciplines. |
| [MMStar](https://mmstar-benchmark.github.io/#Leaderboard) | MMStar is a benchmark to evaluate the multi-modal capacities of vision-language models. |
| [OCRBench](https://huggingface.co/spaces/echo840/ocrbench-leaderboard) | OCRBench is a benchmark to assess the OCR capabilities of multimodal models. |
| [Open Parti Prompts Leaderboard](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard) | Open Parti Prompts Leaderboard compares text-to-image models to each other according to human preferences. |
| [OpenVLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard) | OpenVLM Leaderboard is a platform to evaluate and compare the performance of vision-language models. |
| [PCA-Bench](https://github.com/pkunlp-icler/PCA-EVAL) | PCA-Bench is a benchmark to evaluate the embodied decision-making capabilities of multimodal models. |
| [Q-Bench](https://huggingface.co/spaces/q-future/Q-Bench-Leaderboard) | Q-Bench is a benchmark to evaluate the visual reasoning capabilities of vision-language models. |
| [ReForm-Eval](https://github.com/FudanDISC/ReForm-Eval) | ReForm-Eval is a benchmark to evaluate the visual reasoning capabilities of vision-language models. |
| [RewardBench](https://huggingface.co/spaces/allenai/reward-bench) | RewardBench is a benchmark to evaluate the capabilities and safety of reward models. |
| [ScienceQA](https://scienceqa.github.io/leaderboard.html) | ScienceQA is a benchmark to evaluate the multi-hop reasoning ability and interpretability of AI systems in the context of science question answering. |
| [SciGraphQA](https://github.com/findalexli/SciGraphQA) | SciGraphQA is a benchmark to evaluate the vision-language models in scientific graph question-answering. |
| [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) | SEED-Bench is a benchmark to evaluate the text and image generation of multimodal models. |
| [TextSynth Server](https://bellard.org/ts_server) | TextSynth Server evaluates and compares the performance of language models in generating tokens or images under specific hardware. |
| [TouchStone](https://github.com/ofa-sys/touchstone) | TouchStone is a benchmark to evaluate the overall text-image dialogue capability and alignment level with humans of multimodal models. |
| [URIAL Bench](https://huggingface.co/spaces/allenai/URIAL-Bench) | URIAL Bench is a benchmark to evaluate the capacity of language models for alignment without introducing the factors of fine-tuning (learning rate, data, etc.), which are hard to control for fair comparisons. |
| [Vibe-Eval](https://github.com/reka-ai/reka-vibe-eval) | Vibe-Eval is a benchmark to evaluate vision-language models for challenging cases. |
| [VisIT-Bench](https://visit-bench.github.io) | VisIT-Bench is a benchmark to evaluate the instruction-following capabilities of vision-language models for real-world use. |
| [VisualWebArena](https://jykoh.com/vwa) | VisualWebArena is a benchmark to assess the performance of multimodal web agents on realistic visually grounded tasks. |
| [WHOOPS!](https://huggingface.co/spaces/nlphuji/WHOOPS-Leaderboard-Full) | WHOOPS! is a benchmark to evaluate the visual commonsense of vision-language models. |
| [WildBench](https://huggingface.co/spaces/allenai/WildBench) | WildBench is a benchmark for evaluating language models on challenging tasks that closely resemble real-world applications. |
| [WildVision Arena Leaderboard](https://huggingface.co/spaces/WildVision/vision-arena) | WildVision Arena Leaderboard hosts the chatbot arena, where various vision-language models compete based on their performance in visual understanding. |

# Code

| Name | Description |
| ---- | ----------- |
| [Berkeley Function Calling Leaderboard](https://huggingface.co/spaces/gorilla-llm/berkeley-function-calling-leaderboard) | Berkeley Function Calling Leaderboard evaluates the ability of LLMs to accurately call functions (also known as tools). |
| [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard) | Big Code Models Leaderboard to assess the performance of LLMs on code-related tasks. |
| [BIRD](https://bird-bench.github.io) | BIRD is a benchmark to evaluate the performance of text-to-SQL parsing systems. |
| [CanAiCode Leaderboard](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results) | CanAiCode Leaderboard is a platform to assess the code generation capabilities of LLMs. |
| [Coding LLMs Leaderboard](https://leaderboard.tabbyml.com) | Coding LLMs Leaderboard is a platform to evaluate and rank LLMs across various programming tasks. |
| [CRUXEval](https://crux-eval.github.io/leaderboard.html) | CRUXEval is a benchmark to evaluate code reasoning, understanding, and execution capabilities of LLMs. |
| [CyberSafetyEval](https://huggingface.co/spaces/facebook/CyberSecEval) | CYBERSECEVAL is a benchmark to evaluate the cybersecurity of LLMs as coding assistants. |
| [DevBench](https://github.com/open-compass/devbench) | DevBench is a benchmark to evaluate language models across various stages of the software development lifecycle. |
| [DS-1000](https://ds1000-code-gen.github.io) | DS-1000 is a meta benchmark to evaluate code generation models in data science tasks. |
| [EvalPlus](https://evalplus.github.io/leaderboard.html) | EvalPlus is a benchmark to evaluate the code generation performance of LLMs. |
| [HumanEval.jl](https://github.com/hkust-nlp/felm) | HumanEval.jl is a benchmark to evaluate LLMs' performance with the Julia programming language. |
| [InfiCoder-Eval](https://infi-coder.github.io/inficoder-eval) | InfiCoder-Eval is a benchmark to assess the performance of LLMs in handling code-related tasks. |
| [InterCode](https://intercode-benchmark.github.io) | InterCode is a benchmark to standardize and evaluate interactive coding with execution feedback. |
| [LiveCodeBench](https://huggingface.co/spaces/livecodebench/leaderboard) | LiveCodeBench is a benchmark to evaluate language models across code-related scenarios over time. |
| [Nexus Function Calling Leaderboard](https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard) | InterCode is a benchmark to standardize and evaluate interactive coding with execution feedback. |
| [ODEX](https://code-eval.github.io) | ODEX is a benchmark to evaluate open-domain, multilingual, and execution-based code generation capabilities of LLMs. |
| [OpenEval](https://github.com/NTDXYG/open-eval) | OpenEval is a benchmark to evaluate competition-level code generation of LLMs. |
| [Program Synthesis Models Leaderboard](https://accubits.com/open-source-program-synthesis-models-leaderboard) | Program Synthesis Models Leaderboard provides a ranking and comparison of open-source code models based on their performance. |
| [Spider](https://yale-lily.github.io/spider) | Spider is a benchmark to evaluate the performance of natural language interfaces for cross-domain databases. |
| [SuperCLUE-Code3](https://github.com/CLUEbenchmark/SuperCLUE-Code3) | SuperCLUE-Code3 is a benchmarkto evaluate the performance of LLMs in handling native Chinese programming problems. |
| [SWE-bench](https://www.swebench.com) | SWE-bench is a benchmark for evaluating LLMs on real world software issues collected from GitHub. |

# Math

| Name | Description |
| ---- | ----------- |
| [MathBench](https://open-compass.github.io/MathBench) | MathBench is a multi-level difficulty mathematics evaluation benchmark for language models. |
| [MathVerse](https://mathverse-cuhk.github.io/#leaderboard) | MathVerse is a benchmark to evaluate the capabilities of vision-language models in interpreting and reasoning with visual information in mathematical problems. |
| [MathVista](https://mathvista.github.io/#leaderboard) | MathVista is a benchmark to evaluate mathematical reasoning in visual contexts. |
| [MGSM8KInstruct](https://mathoctopus.github.io) | MGSM8KInstruct is the multilingual mathematical reasoning instruction dataset across ten distinct languages. |
| [MSVAMP](https://mathoctopus.github.io) | MSVAMP is a benchmark to evaluate language models' multilingual mathematical capabilities. |
| [Open Multilingual Reasoning Leaderboard](https://huggingface.co/spaces/kevinpro/Open-Multilingual-Reasoning-Leaderboard) | Open Multilingual Reasoning Leaderboard tracks and ranks the reasoning performance of language models on multilingual mathematical reasoning benchmarks. |
| [SuperCLUE-Math6](https://github.com/CLUEbenchmark/SuperCLUE-Math6) | SuperCLUE-Math6 is a benchmark to evaluate the mathematical reasoning capabilities of Chinese language models. |
| [TabMWP](https://promptpg.github.io/leaderboard.html) | TabMWP is a benchmark to evaluate language models in mathematical reasoning tasks that involve both textual and tabular data. |

# Video

| Name | Description |
| ---- | ----------- |
| [AutoEval-Video](https://huggingface.co/spaces/khhuiyh/AutoEval-Video_LeaderBoard) | AutoEval-Video is a benchmark to assess the capabilities of video models in the context of open-ended video question answering. |
| [MVBench](https://huggingface.co/spaces/OpenGVLab/MVBench_Leaderboard) | MVBench is a benchmark to evaluate the temporal understanding capabilities of video models their in dynamic video tasks. |
| [VBench](https://vchitect.github.io/VBench-project) | VBench is a benchmark to evaluate video generation capabilities of video models. |
| [Video-Bench](https://huggingface.co/spaces/LanguageBind/Video-Bench) | Video-Bench is a benchmark to evaluate the video-exclusive understanding, prior knowledge incorporation, and video-based decision-making abilities of video models. |
| [VLM-Eval](https://github.com/zyayoung/Awesome-Video-LLMs) | VLM-Eval is benchmark to evaluate the video understanding capabilities of video models. |

# Audio

| Name | Description |
| ---- | ----------- |
| [BIGOS](https://huggingface.co/spaces/amu-cai/pl-asr-leaderboard) | BIGOS is a benchmark to evaluate Automatic Speech Recognition (ASR) systems on a diverse set of recordings and their corresponding original transcriptions. |
| [MY Malaysian Speech-to-Text Leaderboard](https://huggingface.co/spaces/mesolitica/malaysian-stt-leaderboard) | MY Malaysian Speech-to-Text Leaderboard aims to track, rank and evaluate Malaysian Speech-to-Text models. |
| [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) | Open ASR Leaderboard provides a platform to tracking, ranking, and evaluating ASR models. |

# 3D

| Name | Description |
| ---- | ----------- |
| [BOP](https://bop.felk.cvut.cz/leaderboards) | BOP is a benchmark to evaluate 6D pose estimation of a rigid object from a single RGB-D input image. |
| [M3DBench](https://m3dbench.github.io) | M3DBench is a benchmark to evaluate the performance of foundation models in understanding multi-modal 3D prompts. |

# Multi-modality

| Name | Description |
| ---- | ----------- |
| [FlagEval](https://flageval.baai.ac.cn/#/trending) | FlagEval is a platform to evaluate foundation models. |
